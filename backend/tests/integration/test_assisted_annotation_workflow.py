"""
Integration test for complete assisted annotation workflow

This test MUST FAIL initially since the endpoints are not implemented.
Tests the complete assisted annotation workflow from quickstart.md Step 3.

Workflow:
1. List and select pre-trained models (YOLO11, YOLO12, RT-DETR, SAM2)
2. Apply assisted annotation using selected model
3. Review and accept/reject model suggestions
4. Combine manual and assisted annotations
5. Test different confidence thresholds
6. Handle model inference errors
"""

import pytest
from io import BytesIO
from PIL import Image
from fastapi.testclient import TestClient
import uuid
import time
from typing import List, Dict

from src.main import app

client = TestClient(app)


class TestAssistedAnnotationWorkflowIntegration:
    """Integration tests for complete assisted annotation workflow."""

    @pytest.fixture(scope="class")
    def setup_test_data(self):
        """Setup test images and ensure models are available."""
        test_data = {
            "image_ids": [],
            "model_ids": {}
        }
        
        # Create and upload test images
        for i in range(2):
            image = Image.new('RGB', (640, 480), color=['red', 'green'][i])
            image_buffer = BytesIO()
            image.save(image_buffer, format='JPEG')
            image_buffer.seek(0)
            
            response = client.post(
                "/api/v1/images",
                files={"files": (f"assist_test_{i}.jpg", image_buffer, "image/jpeg")},
                data={"dataset_split": "train"}
            )
            
            if response.status_code == 201:
                image_data = response.json()["uploaded_images"][0]
                test_data["image_ids"].append(image_data["id"])
        
        # Get available models
        response = client.get("/api/v1/models")
        if response.status_code == 200:
            models = response.json()["models"]
            for model in models:
                model_type = model.get("type", "").lower()
                if model_type in ["detection", "segmentation"]:
                    test_data["model_ids"][model_type] = model["id"]
        
        return test_data

    def test_complete_assisted_annotation_workflow(self, setup_test_data):
        """Test the complete assisted annotation workflow from quickstart Step 3."""
        
        if not setup_test_data["image_ids"]:
            pytest.skip("Image upload not implemented yet")
        
        image_id = setup_test_data["image_ids"][0]
        
        # Step 1: List available models and verify they're displayed
        response = client.get("/api/v1/models")
        
        # This MUST FAIL since endpoint doesn't exist yet
        assert response.status_code == 200
        
        models_response = response.json()
        assert "models" in models_response
        
        # Verify expected model types are available
        model_types = [model["type"] for model in models_response["models"]]
        expected_types = ["detection", "segmentation"]
        
        available_models = {}
        for model in models_response["models"]:
            model_type = model["type"]
            if model_type in expected_types:
                available_models[model_type] = model
        
        assert len(available_models) > 0, "No suitable models found for assisted annotation"
        
        # Step 2: Select SAM2 model for segmentation assistance
        segmentation_model = None
        for model in models_response["models"]:
            if "sam2" in model["name"].lower() or model["type"] == "segmentation":
                segmentation_model = model
                break
        
        if not segmentation_model:
            pytest.skip("SAM2 or segmentation model not available")
        
        # Step 3: Generate assisted annotation
        assisted_annotation_request = {
            "image_id": image_id,
            "model_id": segmentation_model["id"],
            "confidence_threshold": 0.5,
            "max_annotations": 10,
            "annotation_types": ["segments", "bounding_boxes"]
        }
        
        response = client.post("/api/v1/annotations/assisted", json=assisted_annotation_request)
        
        assert response.status_code == 201
        
        assisted_response = response.json()
        
        # Validate assisted annotation structure
        assert assisted_response["image_id"] == image_id
        assert assisted_response["model_id"] == segmentation_model["id"]
        assert assisted_response["creation_method"] == "model"
        assert "suggested_annotations" in assisted_response
        
        # Step 4: Review model suggestions
        suggestions = assisted_response["suggested_annotations"]
        assert len(suggestions) > 0, "No suggestions generated by model"
        
        for suggestion in suggestions:
            assert "confidence" in suggestion
            assert suggestion["confidence"] >= 0.5  # Above threshold
            assert "class_id" in suggestion
            
            # Should have either bounding_box or segment data
            has_bbox = "bounding_box" in suggestion
            has_segment = "segment" in suggestion
            assert has_bbox or has_segment, "Suggestion missing annotation data"

    def test_model_selection_and_variants(self, setup_test_data):
        """Test selection of different model types and variants."""
        
        # List models with filtering by type
        response = client.get("/api/v1/models?type=detection")
        assert response.status_code == 200
        
        detection_models = response.json()["models"]
        
        # Verify YOLO variants are available
        yolo_variants = []
        for model in detection_models:
            if "yolo" in model["name"].lower():
                yolo_variants.append(model)
        
        assert len(yolo_variants) > 0, "No YOLO models found"
        
        # Test different model variants
        for model in yolo_variants[:2]:  # Test first 2 variants
            if not setup_test_data["image_ids"]:
                continue
                
            image_id = setup_test_data["image_ids"][0]
            
            request_data = {
                "image_id": image_id,
                "model_id": model["id"],
                "confidence_threshold": 0.3
            }
            
            response = client.post("/api/v1/annotations/assisted", json=request_data)
            
            if response.status_code == 201:
                result = response.json()
                assert result["model_id"] == model["id"]
                assert "suggested_annotations" in result

    def test_confidence_threshold_effects(self, setup_test_data):
        """Test how confidence thresholds affect annotation suggestions."""
        
        if not setup_test_data["image_ids"] or not setup_test_data["model_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        model_id = list(setup_test_data["model_ids"].values())[0]
        
        # Test different confidence thresholds
        thresholds = [0.3, 0.5, 0.8]
        results = {}
        
        for threshold in thresholds:
            request_data = {
                "image_id": image_id,
                "model_id": model_id,
                "confidence_threshold": threshold
            }
            
            response = client.post("/api/v1/annotations/assisted", json=request_data)
            
            if response.status_code == 201:
                result = response.json()
                results[threshold] = len(result["suggested_annotations"])
        
        # Higher thresholds should generally produce fewer suggestions
        if len(results) >= 2:
            thresholds_sorted = sorted(results.keys())
            for i in range(len(thresholds_sorted) - 1):
                low_threshold = thresholds_sorted[i]
                high_threshold = thresholds_sorted[i + 1]
                # Higher confidence threshold should have <= suggestions
                assert results[high_threshold] <= results[low_threshold]

    def test_accept_reject_suggestions_workflow(self, setup_test_data):
        """Test accepting and rejecting model suggestions."""
        
        if not setup_test_data["image_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        
        # First, generate assisted annotations
        if setup_test_data["model_ids"]:
            model_id = list(setup_test_data["model_ids"].values())[0]
        else:
            pytest.skip("No models available")
        
        response = client.post("/api/v1/annotations/assisted", json={
            "image_id": image_id,
            "model_id": model_id,
            "confidence_threshold": 0.4
        })
        
        if response.status_code != 201:
            pytest.skip("Assisted annotation not working")
        
        suggestions = response.json()["suggested_annotations"]
        
        if len(suggestions) == 0:
            pytest.skip("No suggestions generated")
        
        # Accept first suggestion, reject second (if exists)
        accepted_suggestions = [suggestions[0]]
        rejected_suggestions = suggestions[1:2] if len(suggestions) > 1 else []
        
        # Create final annotation with accepted suggestions
        annotation_data = {
            "image_id": image_id,
            "class_labels": ["detected_object"],
            "user_tag": "assisted_annotation_test",
            "creation_method": "assisted",
            "model_id": model_id,
            "accepted_suggestions": accepted_suggestions,
            "rejected_suggestions": rejected_suggestions
        }
        
        # Convert model suggestions to annotation format
        if "bounding_box" in accepted_suggestions[0]:
            bbox = accepted_suggestions[0]["bounding_box"]
            annotation_data["bounding_boxes"] = [{
                "x": bbox["x"],
                "y": bbox["y"],
                "width": bbox["width"],
                "height": bbox["height"],
                "class_id": accepted_suggestions[0]["class_id"],
                "confidence": accepted_suggestions[0]["confidence"]
            }]
        
        response = client.post("/api/v1/annotations", json=annotation_data)
        assert response.status_code == 201
        
        annotation_response = response.json()
        assert annotation_response["creation_method"] == "assisted"
        assert annotation_response["model_id"] == model_id

    def test_combining_manual_and_assisted_annotations(self, setup_test_data):
        """Test combining manual annotations with assisted suggestions."""
        
        if not setup_test_data["image_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        
        # Create manual annotation first
        manual_annotation = {
            "image_id": image_id,
            "bounding_boxes": [
                {
                    "x": 100.0,
                    "y": 100.0,
                    "width": 150.0,
                    "height": 120.0,
                    "class_id": 0,
                    "confidence": 1.0
                }
            ],
            "class_labels": ["manual_object"],
            "user_tag": "manual_annotator"
        }
        
        response = client.post("/api/v1/annotations", json=manual_annotation)
        if response.status_code != 201:
            pytest.skip("Manual annotation not working")
        
        manual_annotation_id = response.json()["id"]
        
        # Generate assisted annotations for same image
        if setup_test_data["model_ids"]:
            model_id = list(setup_test_data["model_ids"].values())[0]
            
            response = client.post("/api/v1/annotations/assisted", json={
                "image_id": image_id,
                "model_id": model_id,
                "confidence_threshold": 0.5,
                "exclude_overlapping": True  # Avoid overlapping with manual annotations
            })
            
            if response.status_code == 201:
                assisted_suggestions = response.json()["suggested_annotations"]
                
                # Accept some assisted suggestions
                if assisted_suggestions:
                    assisted_annotation = {
                        "image_id": image_id,
                        "class_labels": ["assisted_object"],
                        "user_tag": "assisted_annotator",
                        "creation_method": "assisted",
                        "model_id": model_id
                    }
                    
                    # Add the first suggestion as bounding box
                    if "bounding_box" in assisted_suggestions[0]:
                        bbox = assisted_suggestions[0]["bounding_box"]
                        assisted_annotation["bounding_boxes"] = [{
                            "x": bbox["x"],
                            "y": bbox["y"],
                            "width": bbox["width"],
                            "height": bbox["height"],
                            "class_id": assisted_suggestions[0]["class_id"],
                            "confidence": assisted_suggestions[0]["confidence"]
                        }]
                    
                    response = client.post("/api/v1/annotations", json=assisted_annotation)
                    assert response.status_code == 201
        
        # Verify both annotations exist for the image
        response = client.get(f"/api/v1/annotations?image_id={image_id}")
        assert response.status_code == 200
        
        annotations = response.json()["annotations"]
        
        # Should have both manual and assisted annotations
        creation_methods = [ann["creation_method"] for ann in annotations]
        assert "user" in creation_methods
        assert "assisted" in creation_methods or "model" in creation_methods

    def test_assisted_annotation_error_handling(self, setup_test_data):
        """Test error handling in assisted annotation workflow."""
        
        if not setup_test_data["image_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        
        # Test with invalid model ID
        response = client.post("/api/v1/annotations/assisted", json={
            "image_id": image_id,
            "model_id": "invalid-model-id",
            "confidence_threshold": 0.5
        })
        
        assert response.status_code == 400
        error_data = response.json()
        assert "error" in error_data
        
        # Test with invalid image ID
        response = client.post("/api/v1/annotations/assisted", json={
            "image_id": "invalid-image-id",
            "model_id": list(setup_test_data["model_ids"].values())[0] if setup_test_data["model_ids"] else "test-model",
            "confidence_threshold": 0.5
        })
        
        assert response.status_code == 400
        
        # Test with invalid confidence threshold
        response = client.post("/api/v1/annotations/assisted", json={
            "image_id": image_id,
            "model_id": list(setup_test_data["model_ids"].values())[0] if setup_test_data["model_ids"] else "test-model",
            "confidence_threshold": 1.5  # Invalid - should be <= 1.0
        })
        
        assert response.status_code == 400

    def test_batch_assisted_annotation(self, setup_test_data):
        """Test batch assisted annotation for multiple images."""
        
        if len(setup_test_data["image_ids"]) < 2:
            pytest.skip("Not enough test images")
        
        if not setup_test_data["model_ids"]:
            pytest.skip("No models available")
        
        model_id = list(setup_test_data["model_ids"].values())[0]
        
        batch_request = {
            "image_ids": setup_test_data["image_ids"],
            "model_id": model_id,
            "confidence_threshold": 0.4,
            "max_annotations_per_image": 5
        }
        
        response = client.post("/api/v1/annotations/assisted/batch", json=batch_request)
        
        # This endpoint might not exist yet, so handle both cases
        if response.status_code == 201:
            batch_result = response.json()
            assert "results" in batch_result
            assert len(batch_result["results"]) == len(setup_test_data["image_ids"])
            
            for result in batch_result["results"]:
                assert "image_id" in result
                assert result["image_id"] in setup_test_data["image_ids"]
                assert "suggested_annotations" in result
        
        elif response.status_code == 404:
            # Fallback to individual requests
            for image_id in setup_test_data["image_ids"]:
                individual_request = {
                    "image_id": image_id,
                    "model_id": model_id,
                    "confidence_threshold": 0.4
                }
                
                response = client.post("/api/v1/annotations/assisted", json=individual_request)
                if response.status_code == 201:
                    result = response.json()
                    assert result["image_id"] == image_id

    def test_model_specific_parameters(self, setup_test_data):
        """Test model-specific parameters for assisted annotation."""
        
        if not setup_test_data["image_ids"] or not setup_test_data["model_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        
        # Test SAM2-specific parameters (if available)
        for model_name, model_id in setup_test_data["model_ids"].items():
            if "segmentation" in model_name.lower():
                sam_request = {
                    "image_id": image_id,
                    "model_id": model_id,
                    "confidence_threshold": 0.5,
                    "model_parameters": {
                        "points_per_side": 32,
                        "pred_iou_thresh": 0.88,
                        "stability_score_thresh": 0.95,
                        "crop_n_layers": 1
                    }
                }
                
                response = client.post("/api/v1/annotations/assisted", json=sam_request)
                
                if response.status_code == 201:
                    result = response.json()
                    assert "suggested_annotations" in result
                    # SAM2 should produce segments
                    for suggestion in result["suggested_annotations"]:
                        assert "segment" in suggestion or "polygon" in suggestion

    def test_annotation_refinement_workflow(self, setup_test_data):
        """Test refining assisted annotations with manual adjustments."""
        
        if not setup_test_data["image_ids"] or not setup_test_data["model_ids"]:
            pytest.skip("Test data not available")
        
        image_id = setup_test_data["image_ids"][0]
        model_id = list(setup_test_data["model_ids"].values())[0]
        
        # Generate initial assisted annotation
        response = client.post("/api/v1/annotations/assisted", json={
            "image_id": image_id,
            "model_id": model_id,
            "confidence_threshold": 0.5
        })
        
        if response.status_code != 201:
            pytest.skip("Assisted annotation not working")
        
        suggestions = response.json()["suggested_annotations"]
        
        if not suggestions:
            pytest.skip("No suggestions generated")
        
        # Accept suggestion but refine it manually
        original_suggestion = suggestions[0]
        
        if "bounding_box" in original_suggestion:
            original_bbox = original_suggestion["bounding_box"]
            
            # Create refined annotation with adjusted coordinates
            refined_annotation = {
                "image_id": image_id,
                "bounding_boxes": [
                    {
                        "x": original_bbox["x"] + 5,  # Slight adjustment
                        "y": original_bbox["y"] + 5,
                        "width": original_bbox["width"] - 10,
                        "height": original_bbox["height"] - 10,
                        "class_id": original_suggestion["class_id"],
                        "confidence": 1.0  # User refined, so high confidence
                    }
                ],
                "class_labels": ["refined_object"],
                "user_tag": "refined_annotation",
                "creation_method": "refined",
                "original_model_id": model_id,
                "refinement_notes": "Adjusted bounding box coordinates"
            }
            
            response = client.post("/api/v1/annotations", json=refined_annotation)
            assert response.status_code == 201
            
            refined_result = response.json()
            assert refined_result["creation_method"] == "refined"